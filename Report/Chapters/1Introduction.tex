\section{Background}

“An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.” D. Hawkins. Identification of Outliers, Chapman and Hall, 1980.

\subsection{Applications}
Sensor data, mechanical systems diagnosis, medical data, network intrusion data, newswire posts or financial posts

\subsection{Types of anomalies}

%%
Contextual - values at specific time stamps suddenly change with respect to their temporally adjacent values. 

Collective - entire time series or large subsequences within a time series have unusual shapes \cite{Aggarwal2013a}
%%

\subsection{Aggarwal}

\textbf{9.2 Prediction-Based Outlier Detection in Streaming Time Series}

For contextual anomalies. The most common application of temporal outlier detection is that of detecting deviation-based outliers of specific time-instants with the use of regression-based forecasting models.  (p.276)

\textit{Correlations across time:} This is the same principle as temporal continuity, which is typically implemented using autoregressive modeling and forecasting. Significant deviations from the expected (i.e., forecasted) predictions are defined as outliers. Such significant deviations are therefore defined by violations of temporal continuity. (p.276)

\textit{Correlations across series:} Many sensor applications result in time series that are often closely correlated with one another. For example, a bird call at one sensor will typically also be recorded by a nearby sensor. In such cases, one series can frequently be used in order to predict another. Deviations from such expected predictions can be reported as outliers. (p.276)

Autoregressive models, with focus on multivariate models in 9.2.2. These are: \newline
* VARIMA (9.2.2.1, p.279), can be slow \newline
* PCA-based techniques (9.2.2.3, p. 282), generally more robust to the presence of noise and outliers. Most well-known is SPIRIT.

\textbf{9.3 Time-Series of Unusual Shapes}

For collective anomalies.

\textit{Full-series anomaly:} In this case, the shape of the entire series is treated as an anomaly. This shape is compared against a database of similar time series. However, in most cases, unless the database of sequences corresponds to a relatively short segment of time-stamps, the noise variations within the series will mask the anomalous shape. This is analogous to the problems of noise encountered in detecting outliers in high- dimensional data. (p.287)

\textit{Subsequence-based anomaly:} If the time series is collected over long periods of time, then we might have a single time series that shows typical trends over shorter time-periods. Therefore, the anomalous shape is detected over small windows of the time series as deviations from these typical trends. (p.287

\textit{Methods: }
Transformation to Other Representations (9.3.1)
Distance-Based Methods (9.3.2)
Probabilistic Models (9.3.3)
Linear Models (9.3.4)



\textbf{Vertical analysis}
In time-series analysis, vertical analysis is more important where each individual series (or dimension) is treated as a unit, and the analysis is primarily performed on this unit. In the event that multiple series are available, cross-correlations may be leveraged, although they typically play a secondary role to the analysis of each individual series. This is because time-series data is contextual, which imposes strong temporal locality on series values.  (p.273)

\textbf{Labels}
Labels may be available to supervise the anomaly detection process in the time-series detection settings. In the time-series setting, the labels may be associated with time-instants, with time intervals, or they may be associated with the entire series. (p.275)

\textbf{Supervised vs. unsupervised} 
In general, supervised methods almost always perform better than unsupervised methods because of their ability to discover application-specific abnormalities. The general recommendation is to always use supervision when it is available. (p.275)

Generally, unsupervised methods can be used either for noise removal or anomaly detection, and supervised methods are designed for application-specific anomaly detection. Unsupervised methods are often used in an exploratory setting, where the discovered outliers are provided to the analyst for further examination of their application-specific importance. (p.4)

\textbf{Noise}
In the unsupervised scenario, where previous examples of interesting anomalies are not available, the noise represents the semantic boundary between normal data and true anomalies – noise is often modeled as a weak form of outliers that does not always meet the strong criteria necessary for a data point to be considered interesting or anomalous enough. (p.3)

\textbf{Relationship between Unsupervised Outlier Detection and
Prediction}
The methods in this section use supervised prediction and forecasting methods for unsupervised outlier detection. Outliers are, after all, violations of the “normal” model of data dependencies. A prediction model, therefore, helps in modeling these dependencies as they apply to a specific data point. Violations of these dependencies represent violation of the model of normal data and therefore correspond to outliers. (p.284)