Inspiration:
\begin{itemize}
    \item \url{http://www.nada.kth.se/~ann/exjobb/maxim_wolpher.pdf}
    \item \cite{Aggarwal2013a}
\end{itemize}

\section*{Overview}

\subsection{Prediction-based}

\begin{itemize}
    \item VARIMA \cite{Aggarwal2013a}
    \item Neural Networks (CNN/RNN/LSTM/GRU)
    \item Using PCA to get univariate forecasting \cite{Aggarwal2013a}

\end{itemize}

\subsection{Distance and density-based}
\begin{itemize}
  
    \item Dynamic Time Warping \cite{Aggarwal2013a}
    \item Mahalanobis distance \cite{Aggarwal2013a}
    \item Isolation Forest \cite{Liu2008}
\end{itemize}

\subsection{Probabilistic}

\begin{itemize}
    \item Hidden Markov Model \cite{Aggarwal2013a}
    \item Dynamic Bayesian Networks

\end{itemize}
\subsection{Linear Model}

\begin{itemize}
    \item Principal Component Analysis (PCA) \cite{Li2019a}
    \item Partial Least Squares (PLS) \cite{Li2019a}
    \item Matrix Factorization \cite{Aggarwal2013a}
    \item Support Vector Machines \cite{Aggarwal2013a}
\end{itemize}

\subsection{Deep Learning}

\begin{itemize}
    \item Auto Encoder (AE) \cite{Li2019a}
    \item Deep Autoencoding Gaussian Mixture Model (DAGMM) \cite{Li2019a}
    \item LSTM Encoder-Decoder (LSTM-ED) \cite{Li2019a}
    \item Generative Adversarial Networks (GAN) \cite{Li2019a}
    \item Hierarchical Temporal Memory
\end{itemize}

\subsection{Transformation to other representations}

\begin{itemize}
    \item Leveraging Trajectory Representations of Time Series \cite{Aggarwal2013a}
    \item Numeric Multidimensional Transformations \cite{Aggarwal2013a}
    \item Discrete Sequence Transformations \cite{Aggarwal2013a}
\end{itemize}

\newpage

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%


\section{Prediction-based}

Supervised regression techniques for predicting time-series and then finding the contextual anomalies as deviations from the predicted value is a common method. Traditional methods such as ARIMA and VARIMA can be used for the forecasting, but recently deep learning methods for predicting the values have been explored. In the multivariate case PCA is often used to reduce the problem to a univariate problem.

\subsection{VARIMA} The following is cut from chapter 9.2 in \cite{Aggarwal2013a}. \newline

"By predicting the next value in the series contextual anomalies can be found by comparing the predicted value with the measured. This can be seen as a supervised method for unsupervised anomaly detection. The basic idea in multivariate auto-regressive models is to predict the values at each time-stamp with the past window of length p. The main difference from uni-variate regressive models is that the value at each time-stamp (for any particular series) is predicted as a linear function of all the d Â· p values in all the streams in the previous window of length p.

One problem with the approach is that of increased computational complexity because of the inversion of a matrix. How can one use the basic principle of multivariate regression for forecasting, while keeping the complexity to a manageable level? Two such methods have been proposed in the literature:

\begin{enumerate}
    \item One can select a subset of streams in order to perform the regression with respect to a smaller set of variables. (9.2.2.2)
    \item A fundamentally different approach is to use the notion of hidden variables to decompose the multivariate forecasting problem into a (more easily solvable) set of uni-variate forecasting problems. (9.2.2.3)"
\end{enumerate}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

\subsection{Neural networks} Neural networks have been effective at predicting time-series. Especially recurrent neural networks. Information covering this:
\begin{itemize}
    \item LSTM-based Encoder-Decoder for Multi-Sensor Anomaly Detection \cite{Malhotra2016a}.
    \item Multivariate Time Series Forecasting with LSTMs in Keras \newline \url{https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/}
    \item DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series \cite{Munir2019DeepAnT:Series}
\end{itemize}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%

\subsection{PCA-based methods} These sources present methods and ideas of PCA-based methods for anomaly detection:
\begin{itemize}
    \item Streaming Pattern Discovery in Multiple Time-Series \cite{Papadimitriou2005StreamingTime-series}
\end{itemize}

Regression modelling is more susceptible to noise and outliers. PCA-based methods are generally more robust. They are able to express a large number of correlated data streams into a small number of uncorrelated data streams. \cite{Aggarwal2013a}. Any forecasting model can be applied to the reduced hidden variables. This reduces the time and space complexity by orders of magnitude, because typical forecasting methods are quadratic or worse on the number of variables. Space complexity for multivariate AR is $\bigO{(n^3l^2)}$, where $l$ is the auto-regression window length. For AR per stream (ignoring correlations), it is $\bigO{(nl^2)}$. However, for SPIRIT, we need $\bigO{(kn)}$ space, with one AR model per stream, the total space complexity is $\bigO{(kn+kl^2)}$. \cite{Papadimitriou2005StreamingTime-series}

%%%%%%%%%%%%%%%%%%%

\section{Deep learning}

\subsection{Generative Adversarial Networks (GAN)}

Time complexity:


Short:
Generative Adversarial Networks have recently been successfully used in image processing. A few studies have been made with good results for time series applications [source]. The unsupervised nature of GANs makes it a good model for tasks like anomaly detection.

Description:
A GAN consists of a generator and a discriminator. The generator is generating fake data to fool the discriminator and the discriminator is trying to detect if the generated data is fake. In [source] a framework for anomaly detection of multivariate time series is proposed. In the framework the generator takes its inputs from a random latent space. The entire variable set is treated concurrently to capture the interactions between the variables. Before discrimination the time series is divided into subsets with a sliding window. Based on the outputs of the discriminator, both the parameters of the discriminator and the generator are updated. The generator is exploited by reconstruction, the residuals between the reconstructed data and real-time testing samples. The discriminator is exploited by classifying the time series. A DR-score which is combining the two losses is used to detect potential anomalies.


For:
Works well for finding collective anomalies. Could be expensive.



%%%%%%%%%%%%%%%%%%%

\section{Distance and density-based}

\subsection{Isolation Forest }

Time complexity: 
Linear

Description: 
Builds an ensemble of isolation trees for a given dataset, then anomalies are those instances which have short average paths. Two variables in the method, number of trees and sub-sampling size. It uses no distance or density-based measure and thus eliminates the high computational cost. It has the capacity to scale up to handle extremely large data size and high-dimensional problems with a large number of irrelevant attributes. 

For: 
Works very good for point anomaly detection since it isolates single points easily. Performs better than LOF and random forests for this purpose. It could also be used on statistical features after windowing for finding collective anomalies.

\section{Simulation of data}
Several instances were generated using the statsmodel package in Python. 
Supposed to look like real data. If I have time, try block bootstrapping for making data that resembles the specific use case.

Possible building blocks:
\begin{itemize}
\item Vibration sensors (involving frequencies)
\item Temperature sensors 
\item Proximity sensors
\item Flow sensors
\item Pressure sensors
\item Level sensors
\item Collective anomalies (appearing with specified frequency)
\item Contextual anomalies (appearing with specified frequency)
\end{itemize}
All sensors should be possible to add as clustered sensors. Lag between specific sensors should be possible to add. To begin with no frequency variations will be applied. 

\section{Generation of data}
Think about.


