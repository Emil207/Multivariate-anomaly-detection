%% General methods


%% Reconstruction-based approaches







%% Aggarwal  %%

% 9.2 Prediction-Based Outlier Detection in Streaming Time Series

%For contextual anomalies. The most common application of temporal outlier detection is that of detecting deviation-based outliers of specific time-instants with the use of regression-based forecasting models.  (p.276)

%\textit{Correlations across time:} This is the same principle as temporal continuity, which is typically implemented using autoregressive modeling and forecasting. Significant deviations from the expected (i.e., forecasted) predictions are defined as outliers. Such significant deviations are therefore defined by violations of temporal continuity. (p.276)

%\textit{Correlations across series:} Many sensor applications result in time series that are often closely correlated with one another. For example, a bird call at one sensor will typically also be recorded by a nearby sensor. In such cases, one series can frequently be used in order to predict another. Deviations from such expected predictions can be reported as outliers. (p.276)

%Autoregressive models, with focus on multivariate models in 9.2.2. These are: \newline
%* VARIMA (9.2.2.1, p.279), can be slow \newline
%* PCA-based techniques (9.2.2.3, p. 282), generally more robust to the presence of noise and outliers. Most well-known is SPIRIT.

%\textbf{9.3 Time-Series of Unusual Shapes}

%For collective anomalies.

%\textit{Full-series anomaly:} In this case, the shape of the entire series is treated as an anomaly. This shape is compared against a database of similar time series. However, in most cases, unless the database of sequences corresponds to a relatively short segment of time-stamps, the noise variations within the series will mask the anomalous shape. This is analogous to the problems of noise encountered in detecting outliers in high- dimensional data. (p.287)

%\textit{Subsequence-based anomaly:} If the time series is collected over long periods of time, then we might have a single time series that shows typical trends over shorter time-periods. Therefore, the anomalous shape is detected over small windows of the time series as deviations from these typical trends. (p.287

%\textit{Methods: }
%Transformation to Other Representations (9.3.1)
%Distance-Based Methods (9.3.2)
%Probabilistic Models (9.3.3)
%Linear Models (9.3.4)



%\textbf{Vertical analysis}
%In time-series analysis, vertical analysis is more important where each individual series (or dimension) is treated as a unit, and the analysis is primarily performed on this unit. In the event that multiple series are available, cross-correlations may be leveraged, although they typically play a secondary role to the analysis of each individual series. This is because time-series data is contextual, which imposes strong temporal locality on series values.  (p.273)

%\textbf{Labels}
%Labels may be available to supervise the anomaly detection process in the time-series detection settings. In the time-series setting, the labels may be associated with time-instants, with time intervals, or they may be associated with the entire series. (p.275)

%\textbf{Supervised vs. unsupervised} 
%In general, supervised methods almost always perform better than unsupervised methods because of their ability to discover application-specific abnormalities. The general recommendation is to always use supervision when it is available. (p.275)

%Generally, unsupervised methods can be used either for noise removal or anomaly detection, and supervised methods are designed for application-specific anomaly detection. Unsupervised methods are often used in an exploratory setting, where the discovered outliers are provided to the analyst for further examination of their application-specific importance. (p.4)

%\textbf{Noise}
%In the unsupervised scenario, where previous examples of interesting anomalies are not available, the noise represents the semantic boundary between normal data and true anomalies – noise is often modeled as a weak form of outliers that does not always meet the strong criteria necessary for a data point to be considered interesting or anomalous enough. (p.3)

%\textbf{Relationship between Unsupervised Outlier Detection and
%Prediction}
%The methods in this section use supervised prediction and forecasting methods for unsupervised outlier detection. Outliers are, after all, violations of the “normal” model of data dependencies. A prediction model, therefore, helps in modeling these dependencies as they apply to a specific data point. Violations of these dependencies represent violation of the model of normal data and therefore correspond to outliers. (p.284)


%Questions to be answered:
%How could one classify different types of anomalies?
%How to distinguish between noise and anomalies in unsupervised methods?
%The difference of being in an online and offline setting?
%How does deep learning based methods compare in complexity?

%Inspiration:
%\begin{itemize}
%    \item \url{http://www.nada.kth.se/~ann/exjobb/maxim_wolpher.pdf}
%    \item \cite{Aggarwal2013a}
%\end{itemize}

\section*{Overview}

\subsection{Prediction-based}

\begin{itemize}
    \item VARIMA %\cite{Aggarwal2013a}
    \item Neural Networks (CNN/RNN/LSTM/GRU)
    \item Using PCA to get univariate forecasting %\cite{Aggarwal2013a}

\end{itemize}

\subsection{Distance and density-based}
\begin{itemize}
  
    \item Dynamic Time Warping %\cite{Aggarwal2013a}
    \item Mahalanobis distance %\cite{Aggarwal2013a}
    \item Isolation Forest %\cite{Liu2008}
\end{itemize}

\subsection{Probabilistic}

\begin{itemize}
    \item Hidden Markov Model %\cite{Aggarwal2013a}
    \item Dynamic Bayesian Networks

\end{itemize}
\subsection{Linear Model}

\begin{itemize}
    \item Principal Component Analysis (PCA) %\cite{Li2019a}
    \item Partial Least Squares (PLS) %\cite{Li2019a}
    \item Matrix Factorization %\cite{Aggarwal2013a}
    \item Support Vector Machines %\cite{Aggarwal2013a}
\end{itemize}

\subsection{Deep Learning}

\begin{itemize}
    \item Auto Encoder (AE) %\cite{Li2019a}
    \item Deep Autoencoding Gaussian Mixture Model (DAGMM) %\cite{Li2019a}
    \item LSTM Encoder-Decoder (LSTM-ED) %\cite{Li2019a}
    \item Generative Adversarial Networks (GAN) %\cite{Li2019a}
    \item Hierarchical Temporal Memory
\item Variational Autencoder (VAE)
\end{itemize}

\subsection{Transformation to other representations}

\begin{itemize}
    \item Leveraging Trajectory Representations of Time Series %\cite{Aggarwal2013a}
    \item Numeric Multidimensional Transformations %\cite{Aggarwal2013a}
    \item Discrete Sequence Transformations %\cite{Aggarwal2013a}
\end{itemize}

\newpage

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%


%\section{Prediction-based}

%Supervised regression techniques for predicting time-series and then finding the contextual anomalies as deviations from the predicted value is a common method. Traditional methods such as ARIMA and VARIMA can be used for the forecasting, but recently deep learning methods for predicting the values have been explored. In the multivariate case PCA is often used to reduce the problem to a univariate problem.

%\subsection{VARIMA} The following is cut from chapter 9.2 in \cite{Aggarwal2013a}. \newline

%"By predicting the next value in the series contextual anomalies can be found by comparing the predicted value with the measured. This can be seen as a supervised method for unsupervised anomaly detection. The basic idea in multivariate auto-regressive models is to predict the values at each time-stamp with the past window of length p. The main difference from uni-variate regressive models is that the value at each time-stamp (for any particular series) is predicted as a linear function of all the d · p values in all the streams in the previous window of length p.

%One problem with the approach is that of increased computational complexity because of the inversion of a matrix. How can one use the basic principle of multivariate regression for forecasting, while keeping the complexity to a manageable level? Two such methods have been proposed in the literature:

%\begin{enumerate}
 %   \item One can select a subset of streams in order to perform the regression with respect to a smaller set of variables. (9.2.2.2)
 %   \item A fundamentally different approach is to use the notion of hidden variables to decompose the multivariate forecasting problem into a (more easily solvable) set of uni-variate forecasting problems. (9.2.2.3)"
%\end{enumerate}

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%

%\subsection{Neural networks} Neural networks have been effective at predicting time-series. Especially recurrent neural networks. Information covering this:
%\begin{itemize}
%   \item LSTM-based Encoder-Decoder for Multi-Sensor Anomaly Detection \cite{Malhotra2016a}.
%    \item Multivariate Time Series Forecasting with LSTMs in Keras \newline \url{https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/}
 %   \item DeepAnT: A Deep Learning Approach for Unsupervised Anomaly Detection in Time Series \cite{Munir2019DeepAnT:Series}
%\end{itemize}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%

%\subsection{PCA-based methods} These sources present methods and ideas of PCA-based methods for anomaly detection:
%\begin{itemize}
%    \item Streaming Pattern Discovery in Multiple Time-Series \cite{Papadimitriou2005StreamingTime-series}
%\end{itemize}

%Regression modelling is more susceptible to noise and outliers. PCA-based methods are generally more robust. They are able to express a large number of correlated data streams into a small number of uncorrelated data streams. \cite{Aggarwal2013a}. Any forecasting model can be applied to the reduced hidden variables. This reduces the time and space complexity by orders of magnitude, because typical forecasting methods are quadratic or worse on the number of variables. Space complexity for multivariate AR is $\bigO{(n^3l^2)}$, where $l$ is the auto-regression window length. For AR per stream (ignoring correlations), it is $\bigO{(nl^2)}$. However, for SPIRIT, we need $\bigO{(kn)}$ space, with one AR model per stream, the total space complexity is $\bigO{(kn+kl^2)}$. \cite{Papadimitriou2005StreamingTime-series}

%%%%%%%%%%%%%%%%%%%

%\section{Deep learning}

%\subsection{Generative Adversarial Networks (GAN)}

%Time complexity:


%Short:
%Generative Adversarial Networks have recently been successfully used in image processing. A few studies have been made with good results for time series applications [source]. The unsupervised nature of GANs makes it a good model for tasks like anomaly detection.

%Description:
%A GAN consists of a generator and a discriminator. The generator is generating fake data to fool the discriminator and the discriminator is trying to detect if the generated data is fake. In [source] a framework for anomaly detection of multivariate time series is proposed. In the framework the generator takes its inputs from a random latent space. The entire variable set is treated concurrently to capture the interactions between the variables. Before discrimination the time series is divided into subsets with a sliding window. Based on the outputs of the discriminator, both the parameters of the discriminator and the generator are updated. The generator is exploited by reconstruction, the residuals between the reconstructed data and real-time testing samples. The discriminator is exploited by classifying the time series. A DR-score which is combining the two losses is used to detect potential anomalies.


%For:
%Works well for finding collective anomalies. Could be expensive.

%%%%%%%%%%%%%%%%%%%

%\section{Distance and density-based}

%\subsection{Isolation Forest }

%Time complexity: 
%Linear

%Description: 
%Builds an ensemble of isolation trees for a given dataset, then anomalies are those instances which have short average paths. Two variables in the method, number of trees and sub-sampling size. It uses no distance or density-based measure and thus eliminates the high computational cost. It has the capacity to scale up to handle extremely large data size and high-dimensional problems with a large number of irrelevant attributes. 

%For: 
%Works very good for point anomaly detection since it isolates single points easily. Performs better than LOF and random forests for this purpose. It could also be used on statistical features after windowing for finding collective anomalies.



