Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Mirza2014,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Mirza, Osindero - Unknown - Conditional Generative Adversarial Nets.pdf:pdf},
title = {{Conditional Generative Adversarial Nets}},
url = {https://arxiv.org/pdf/1411.1784.pdf http://arxiv.org/abs/1411.1784},
year = {2014}
}
@inproceedings{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.3156/jsoft.29.5_177_2},
eprint = {1406.2661},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf},
issn = {10495258},
number = {January},
pages = {2672--2680},
title = {{Generative adversarial nets}},
url = {http://www.github.com/goodfeli/adversarial},
volume = {3},
year = {2014}
}
@inproceedings{Song2017,
abstract = {Existing Markov Chain Monte Carlo (MCMC) methods are either based on generalpurpose and domain-agnostic schemes, which can lead to slow convergence, or problem-specific proposals hand-crafted by an expert. In this paper, we propose A-NICE-MC, a novel method to automatically design efficient Markov chain kernels tailored for a specific domain. First, we propose an efficient likelihood-free adversarial training method to train a Markov chain and mimic a given data distribution. Then, we leverage flexible volume preserving flows to obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to train efficient Markov chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples. Empirical results demonstrate that A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of deep neural networks, and is able to significantly outperform competing methods such as Hamiltonian Monte Carlo.},
archivePrefix = {arXiv},
arxivId = {1706.07561},
author = {Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.07561},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Song, Zhao, Ermon - Unknown - A-NICE-MC Adversarial Training for MCMC.pdf:pdf},
issn = {10495258},
pages = {5141--5151},
title = {{A-NICE-MC: Adversarial training for MCMC}},
url = {https://pdfs.semanticscholar.org/ff1e/e941c8fe334afd7dcd92b5c4478c0ae16efe.pdf},
volume = {2017-Decem},
year = {2017}
}
@article{DeMeerPardo2019,
abstract = {The scarcity of historical financial data has been a huge hindrance for the development algorithmic trading models ever since the first models were devised. Most financial models assume as hypothesis a series of characteristics regarding the nature of financial time series and seek extracting information about the state of the market through calibration. Through backtesting, a large number of these models are seen not to perform and are thus discarded. The remaining well-performing models however, are highly vulnerable to overfitting. Financial time series are complex by nature and their behaviour changes over time, so this concern is well founded. In addition to the problem of overfitting, available data is far too scarce for most machine learning applications and impossibly scarce for advanced approaches such as reinforcement learning, which has heavily impaired the application of these novel techniques in financial settings. This is where data generation comes into play. Generative Adversarial Networks, GANs, are a type of neural network architecture that focuses on sample generation. Through adversarial training, the GAN can learn the underlying structure of the input data and become able to generate samples very similar to those of the data distribution. This is specially useful in the case of high-dimensional objects, in which the dimensions are heavily inter-dependent, such as images, music and in our case financial time series. In this work we want to explore the generating capabilities of GANs applied to financial time series and investigate whether or not we can generate realistic financial scenarios.},
author = {{De Meer Pardo}, Fernando},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/Enriching{\_}Financial{\_}Datasets{\_}with{\_}Generative{\_}Adversarial{\_}Networks.pdf:pdf},
number = {July},
title = {{Enriching Financial Datasets with Generative Adversarial Networks}},
url = {https://repository.tudelft.nl/islandora/object/uuid:51d69925-fb7b-4e82-9ba6-f8295f96705c},
year = {2019}
}
@article{Goodfellow2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
eprint = {1701.00160},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Networks(2).pdf:pdf},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf http://arxiv.org/abs/1701.00160},
year = {2016}
}
@article{Li2019a,
abstract = {The prevalence of networked sensors and actuators in many real-world systems such as smart buildings, factories, power plants, and data centers generate substantial amounts of multivariate time series data for these systems. The rich sensor data can be continuously monitored for intrusion events through anomaly detection. However, conventional threshold-based anomaly detection methods are inadequate due to the dynamic complexities of these systems, while supervised machine learning methods are unable to exploit the large amounts of data due to the lack of labeled data. On the other hand, current unsupervised machine learning approaches have not fully exploited the spatial-temporal correlation and other dependencies amongst the multiple variables (sensors/actuators) in the system for detecting anomalies. In this work, we propose an unsupervised multivariate anomaly detection method based on Generative Adversarial Networks (GANs). Instead of treating each data stream independently, our proposed MAD-GAN framework considers the entire variable set concurrently to capture the latent interactions amongst the variables. We also fully exploit both the generator and discriminator produced by the GAN, using a novel anomaly score called DR-score to detect anomalies by discrimination and reconstruction. We have tested our proposed MAD-GAN using two recent datasets collected from real-world CPS: the Secure Water Treatment (SWaT) and the Water Distribution (WADI) datasets. Our experimental results showed that the proposed MAD-GAN is effective in reporting anomalies caused by various cyber-intrusions compared in these complex real-world systems.},
archivePrefix = {arXiv},
arxivId = {1901.04997},
author = {Li, Dan and Chen, Dacheng and Shi, Lei and Jin, Baihong and Goh, Jonathan and Ng, See-Kiong},
eprint = {1901.04997},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2019 - MAD-GAN Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks.pdf:pdf},
month = {jan},
title = {{MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1901.04997},
year = {2019}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Chintala, Bottou - 2017 - Wasserstein GAN.pdf:pdf},
month = {jan},
title = {{Wasserstein GAN}},
url = {https://arxiv.org/pdf/1701.07875.pdf http://arxiv.org/abs/1701.07875},
year = {2017}
}
@inproceedings{Gulrajani2017,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1704.00028},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Gulrajani et al. - Unknown - Improved Training of Wasserstein GANs Montreal Institute for Learning Algorithms(2).pdf:pdf},
issn = {10495258},
pages = {5768--5778},
title = {{Improved training of wasserstein GANs}},
url = {https://github.com/igul222/improved{\_}wgan{\_}training.},
volume = {2017-Decem},
year = {2017}
}
@article{Doan2019,
abstract = {Generative Adversarial Networks (GANs) can successfully approximate a probability distribution and produce realistic samples. However, open questions such as sufficient convergence conditions and mode collapse still persist. In this paper, we build on existing work in the area by proposing a novel framework for training the generator against an ensemble of discriminator networks, which can be seen as a one-student/multiple-teachers setting. We formalize this problem within the full-information adversarial bandit framework, where we evaluate the capability of an algorithm to select mixtures of discriminators for providing the generator with feedback during learning. To this end, we propose a reward function which reflects the progress made by the generator and dynamically update the mixture weights allocated to each discriminator. We also draw connections between our algorithm and stochastic optimization methods and then show that existing approaches using multiple discriminators in literature can be recovered from our framework. We argue that less expressive discriminators are smoother and have a general coarse grained view of the modes map, which enforces the generator to cover a wide portion of the data distribution support. On the other hand, highly expressive discriminators ensure samples quality. Finally, experimental results show that our approach improves samples quality and diversity over existing baselines by effectively learning a curriculum. These results also support the claim that weaker discriminators have higher entropy improving modes coverage.},
archivePrefix = {arXiv},
arxivId = {1808.00020},
author = {Doan, Thang and Monteiro, Jo{\~{a}}o and Albuquerque, Isabela and Mazoure, Bogdan and Durand, Audrey and Pineau, Joelle and Hjelm, R. Devon},
doi = {10.1609/aaai.v33i01.33013470},
eprint = {1808.00020},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/4224-Article Text-7278-1-10-20190705.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {3470--3477},
title = {{On-Line Adaptative Curriculum Learning for GANs}},
volume = {33},
year = {2019}
}
@article{Yoon2019a,
abstract = {We deconstruct the performance of GANs into three components: 1. Formulation: we propose a perturbation view of the population target of GANs. Building on this interpretation, we show that GANs can be viewed as a generalization of the robust statistics framework, and propose a novel GAN architecture, termed as Cascade GANs, to provably recover meaningful low-dimensional generator approximations when the real distribution is high-dimensional and corrupted by outliers. 2. Generalization: given a population target of GANs, we design a systematic principle, projection under admissible distance, to design GANs to meet the population requirement using finite samples. We implement our principle in three cases to achieve polynomial and sometimes near-optimal sample complexities: (1) learning an arbitrary generator under an arbitrary pseudonorm; (2) learning a Gaussian location family under TV distance, where we utilize our principle provide a new proof for the optimality of Tukey median viewed as GANs; (3) learning a low-dimensional Gaussian approximation of a high-dimensional arbitrary distribution under Wasserstein distance. We demonstrate a fundamental trade-off in the approximation error and statistical error in GANs, and show how to apply our principle with empirical samples to predict how many samples are sufficient for GANs in order not to suffer from the discriminator winning problem. 3. Optimization: we demonstrate alternating gradient descent is provably not locally asymptotically stable in optimizing the GAN formulation of PCA. We diagnose the problem as the minimax duality gap being non-zero, and propose a new GAN architecture whose duality gap is zero, where the value of the game is equal to the previous minimax value (not the maximin value). We prove the new GAN architecture is globally asymptotically stable in optimization under alternating gradient descent.},
archivePrefix = {arXiv},
arxivId = {1901.09465},
author = {Yoon, Jinsung and Jarrett, Daniel and van der Schaar, Mahaela},
eprint = {1901.09465},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/8789-time-series-generative-adversarial-networks.pdf:pdf},
number = {NeurIPS},
pages = {1--11},
title = {{Time-Series Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1901.09465},
year = {2019}
}
@article{Schlegl2019,
abstract = {Obtaining expert labels in clinical imaging is difficult since exhaustive annotation is time-consuming. Furthermore, not all possibly relevant markers may be known and sufficiently well described a priori to even guide annotation. While supervised learning yields good results if expert labeled training data is available, the visual variability, and thus the vocabulary of findings, we can detect and exploit, is limited to the annotated lesions. Here, we present fast AnoGAN (f-AnoGAN), a generative adversarial network (GAN) based unsupervised learning approach capable of identifying anomalous images and image segments, that can serve as imaging biomarker candidates. We build a generative model of healthy training data, and propose and evaluate a fast mapping technique of new data to the GAN's latent space. The mapping is based on a trained encoder, and anomalies are detected via a combined anomaly score based on the building blocks of the trained model – comprising a discriminator feature residual error and an image reconstruction error. In the experiments on optical coherence tomography data, we compare the proposed method with alternative approaches, and provide comprehensive empirical evidence that f-AnoGAN outperforms alternative approaches and yields high anomaly detection accuracy. In addition, a visual Turing test with two retina experts showed that the generated images are indistinguishable from real normal retinal OCT images. The f-AnoGAN code is available at https://github.com/tSchlegl/f-AnoGAN.},
author = {Schlegl, Thomas and Seeb{\"{o}}ck, Philipp and Waldstein, Sebastian M. and Langs, Georg and Schmidt-Erfurth, Ursula},
doi = {10.1016/j.media.2019.01.010},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/fastAnoGAN.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Anomaly detection,Optical coherence tomography,Unsupervised learning,Wasserstein generative adversarial network},
number = {May},
pages = {30--44},
title = {{f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks}},
volume = {54},
year = {2019}
}
@article{Esteban2017,
abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.},
archivePrefix = {arXiv},
arxivId = {1706.02633},
author = {Esteban, Crist{\'{o}}bal and Hyland, Stephanie L and R{\"{a}}tsch, Gunnar},
eprint = {1706.02633},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Hyland et al. - 2015 - REAL-VALUED (MEDICAL) TIME SERIES GENERA-TION WITH RECURRENT CONDITIONAL GANS.pdf:pdf},
title = {{Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs}},
url = {https://arxiv.org/pdf/1706.02633v2.pdf http://arxiv.org/abs/1706.02633},
year = {2017}
}
@article{Zhang2019,
abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GAXs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work1, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr{\'{e}}het Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
archivePrefix = {arXiv},
arxivId = {1805.08318},
author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
eprint = {1805.08318},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/1805.08318.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {12744--12753},
title = {{Self-attention generative adversarial networks}},
volume = {2019-June},
year = {2019}
}
@inproceedings{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1606.03498},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - Unknown - Improved Techniques for Training GANs.pdf:pdf},
issn = {10495258},
pages = {2234--2242},
title = {{Improved techniques for training GANs}},
url = {https://github.com/openai/},
year = {2016}
}
@article{Mogren2016,
abstract = {Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.},
archivePrefix = {arXiv},
arxivId = {1611.09904},
author = {Mogren, Olof},
eprint = {1611.09904},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Mogren - Unknown - C-RNN-GAN Continuous recurrent neural networks with adversarial training.pdf:pdf},
title = {{C-RNN-GAN: Continuous recurrent neural networks with adversarial training}},
url = {https://github.com/olofmogren/c-rnn-gan http://arxiv.org/abs/1611.09904},
year = {2016}
}
