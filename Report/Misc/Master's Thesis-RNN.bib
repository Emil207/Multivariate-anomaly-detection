Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/naturebp.pdf:pdf},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
month = {oct},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
url = {http://www.nature.com/articles/323533a0},
volume = {323},
year = {1986}
}
@inproceedings{Nanduri2016,
abstract = {Anomaly Detection in multivariate, time-series data collected from aircraft's Flight Data Recorder (FDR) or Flight Operational Quality Assurance (FOQA) data provide a powerful means for identifying events and trends that reduce safety margins. The industry standard Exceedance Detection algorithm uses a list of specified parameters and their thresholds to identify known deviations. In contrast, Machine Learning algorithms detect unknown unusual patterns in the data either through semi-supervised or unsupervised learning. The Multiple Kernel Anomaly Detection (MKAD) algorithm based on One-class SVM identified 6 of 11 canonical anomalies in a large dataset but is limited by the need for dimensionality reduction, poor sensitivity to short term anomalies, and inability to detect anomalies in latent features. This paper describes the application of Recurrent Neural Networks (RNN) with Long Term Short Term Memory (LTSM) and Gated Recurrent Units (GRU) architectures which can overcome the limitations described above. The RNN algorithms detected 9 out the 11 anomalies in the test dataset with Precision = 1, Recall = 0.818 and F1 score = 0.89. RNN architectures, designed for time-series data, are suited for implementation on the flight deck to provide real-time anomaly detection. The implications of these results are discussed.},
author = {Nanduri, Anvardh and Sherry, Lance},
booktitle = {ICNS 2016: Securing an Integrated CNS System to Meet Future Challenges},
doi = {10.1109/ICNSURV.2016.7486356},
isbn = {9781509021499},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Anomaly detection in aircraft data using Recurrent Neural Networks (RNN)}},
year = {2016}
}
@inproceedings{Lipton2015,
abstract = {Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations. For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR). While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data. Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data. They effectively model varying length sequences and capture long range dependencies. We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements. Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements. First, we establish the effectiveness of a simple LSTM network for modeling clinical data. Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step. Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.},
archivePrefix = {arXiv},
arxivId = {1511.03677},
author = {Lipton, Zachary C. and Kale, David C. and Elkan, Charles and Wetzel, Randall},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.03677},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Lipton et al. - 2015 - Learning to Diagnose with LSTM Recurrent Neural Networks.pdf:pdf},
month = {nov},
title = {{Learning to diagnose with LSTM recurrent neural networks}},
url = {https://arxiv.org/abs/1511.03677},
year = {2016}
}
@article{Malhotra2016a,
abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
annote = {Prediction-based on univariate series.},
archivePrefix = {arXiv},
arxivId = {1607.00148},
author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
eprint = {1607.00148},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Malhotra et al. - 2016 - LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection.pdf:pdf},
title = {{LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection}},
url = {http://arxiv.org/abs/1607.00148},
year = {2016}
}
@article{Rumelhart1986b,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:Users/EmilAndersson/Documents/Emil Andersson/Utbildning/Universitet/Exjobb/Multivariate-anomaly-detection/Papers/naturebp.pdf:pdf},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@inproceedings{Malhotra2015a,
abstract = {Long Short Term Memory (LSTM) networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behavior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
author = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
booktitle = {23rd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2015 - Proceedings},
file = {:Users/EmilAndersson/Library/Application Support/Mendeley Desktop/Downloaded/Malhotra et al. - 2015 - Long Short Term Memory Networks for Anomaly Detection in Time Series.pdf:pdf},
isbn = {9782875870148},
pages = {89--94},
title = {{Long Short Term Memory networks for anomaly detection in time series}},
url = {http://www.i6doc.com/en/.},
year = {2015}
}
